import os
import numpy as np
from sklearn.model_selection import train_test_split
from collections import Counter
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import to_categorical
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.preprocessing.image import load_img, img_to_array
import tensorflow as tf
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import matplotlib.pyplot as plt

# Define paths (update this to the correct dataset path)
base_dir = r'C:\Users\sajal\Desktop\dental project materials\dataset_dental_project'  # Update this path

# List all image files and their corresponding classes
image_paths = []
labels = []
class_names = sorted(os.listdir(base_dir))
for class_name in class_names:
    class_dir = os.path.join(base_dir, class_name)
    if os.path.isdir(class_dir):
        for root, _, files in os.walk(class_dir):
            for file in files:
                if file.endswith(('jpg', 'jpeg', 'png')):
                    image_paths.append(os.path.join(root, file))
                    labels.append(class_name)

# Convert to numpy arrays
image_paths = np.array(image_paths)
labels = np.array(labels)

# Split into train, validation, and test sets (80% train, 10% validation, 10% test)
train_paths, temp_paths, train_labels, temp_labels = train_test_split(image_paths, labels, test_size=0.2, stratify=labels, random_state=42)

# Split the temporary set into validation and test sets (50% validation, 50% test)
val_paths, test_paths, val_labels, test_labels = train_test_split(temp_paths, temp_labels, test_size=0.5, stratify=temp_labels, random_state=42)

# Count the number of images per disease class
train_label_counts = Counter(train_labels)
val_label_counts = Counter(val_labels)
test_label_counts = Counter(test_labels)

# Print the counts for each disease class
print("Train set:")
for label, count in train_label_counts.items():
    print(f"{label}: {count} images")

print("\nValidation set:")
for label, count in val_label_counts.items():
    print(f"{label}: {count} images")

print("\nTest set:")
for label, count in test_label_counts.items():
    print(f"{label}: {count} images")

# Encode the labels into integers
label_encoder = LabelEncoder()
train_labels_encoded = label_encoder.fit_transform(train_labels)
val_labels_encoded = label_encoder.transform(val_labels)
test_labels_encoded = label_encoder.transform(test_labels)

# Convert labels to categorical format
train_labels_categorical = to_categorical(train_labels_encoded)
val_labels_categorical = to_categorical(val_labels_encoded)
test_labels_categorical = to_categorical(test_labels_encoded)

# Define a function to generate batches of images and labels
def data_generator(image_paths, labels, batch_size, target_size):
    num_samples = len(image_paths)
    while True:
        for offset in range(0, num_samples, batch_size):
            batch_paths = image_paths[offset:offset + batch_size]
            batch_labels = labels[offset:offset + batch_size]

            images = []
            for image_path in batch_paths:
                img = load_img(image_path, target_size=target_size)
                img = img_to_array(img)
                img = img / 255.0
                images.append(img)

            yield np.array(images), np.array(batch_labels)

# Parameters
batch_size = 32
target_size = (224, 224)

# Apply Data Augmentation
train_datagen = ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

# Load training images
train_images = []
for image_path in train_paths:
    img = load_img(image_path, target_size=target_size)
    img = img_to_array(img)
    train_images.append(img)

# Convert to numpy array
train_images = np.array(train_images)

# Fit the ImageDataGenerator on the training data
train_datagen.fit(train_images)

# Create data generators
train_generator = data_generator(train_paths, train_labels_categorical, batch_size, target_size)
val_generator = data_generator(val_paths, val_labels_categorical, batch_size, target_size)
test_generator = data_generator(test_paths, test_labels_categorical, batch_size, target_size)

# Calculate steps per epoch
train_steps = len(train_paths) // batch_size
val_steps = len(val_paths) // batch_size
test_steps = len(test_paths) // batch_size

# Load the pre-trained ResNet50 model + higher level layers
base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Unfreeze some layers of the base model
for layer in base_model.layers[-10:]:
    layer.trainable = True

# Create a new model on top with Dropout and BatchNormalization
model = Sequential([
    base_model,
    Flatten(),
    Dense(512, activation='relu'),
    Dropout(0.5),
    BatchNormalization(),
    Dense(len(class_names), activation='softmax')
])

# Adjust Learning Rate
model.compile(optimizer=Adam(learning_rate=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])

# Set up early stopping
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Train the model
history = model.fit(
    train_generator,
    steps_per_epoch=train_steps,
    epochs=10,  # Adjust the number of epochs as needed
    validation_data=val_generator,
    validation_steps=val_steps,
    callbacks=[early_stopping]
)

# Evaluate the model on the test set
test_loss, test_accuracy = model.evaluate(test_generator, steps=test_steps)

# Predict probabilities for the test set
y_pred_prob = model.predict(test_generator, steps=test_steps)

# Convert probabilities to class predictions
y_pred = np.argmax(y_pred_prob, axis=1)

# Convert categorical test labels back to integers
test_labels_encoded = np.argmax(test_labels_categorical, axis=1)

# Calculate evaluation metrics
accuracy = accuracy_score(test_labels_encoded, y_pred)
precision = precision_score(test_labels_encoded, y_pred, average='weighted')
recall = recall_score(test_labels_encoded, y_pred, average='weighted')
f1 = f1_score(test_labels_encoded, y_pred, average='weighted')
roc_auc = roc_auc_score(test_labels_categorical, y_pred_prob, average='weighted', multi_class='ovr')

# Display evaluation metrics in tabular form
print("\nTest Loss:", test_loss)
print("Test Accuracy:", test_accuracy)
print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)
print("ROC AUC Score:", roc_auc)

# Plot a graph to visualize the evaluation metrics
metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC Score']
metrics_values = [accuracy, precision, recall, f1, roc_auc]

plt.figure(figsize=(10, 6))
plt.bar(metrics_names, metrics_values, color=['blue', 'green', 'orange', 'red', 'purple'])
plt.title('Evaluation Metrics')
plt.xlabel('Metrics')
plt.ylabel('Values')
plt.grid(axis='y')
plt.show()

# User Input for Model Saving
correct_prediction = input("Was the prediction for the test images correct? (yes/no): ").lower()
if correct_prediction == "yes":
    save_model = input("Do you want to save the model? (yes/no): ").lower()
    if save_model == "yes":
        model.save("dental_disease_model_resnet50.h5")
        print("Model saved successfully.")
